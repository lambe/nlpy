Matrix-Free Augmented Lagrangian TODO List
------------------------------------------

- Absolute and relative convergence tolerances at each level of method

- Counters for calls to original NLP objective, constraint, gradient, and 
jprod and jtprod functions

- Nocedal-Yuan backtracking
	- implemented, May 15, 2012

- Magical steps for slack variables
	- implemented, May 16, 2012

- Nonmonotone update strategy

- Automatic variable / constraint scaling

- Store previous x to prevent repeated function evaluations at the same point
	- early version complete May 16, 2012, more advanced work in progress

- Detect infeasible problems reliably (use Lancelot A strategy?)
	- simple implementation added May 16, 2012


More "radical" / experimental ideas:

- "Smart" selection of initial TR radius for each major iteration

- Use SHA1 key for storing x (see above) for performance
	- early version implemented May 17, 2012 (local branch only)

- "Smart" selection of initial penalty parameter

- "Smart" selection of initial multipliers, e.g. least-squares initialization

- Extrapolation of x near a solution with good multiplier estimates

- More interesting ways of computing the step???